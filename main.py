from fastapi import FastAPI
from pydantic import BaseModel
from ortools.constraint_solver import routing_enums_pb2
from ortools.constraint_solver import pywrapcp
import pandas as pd
import numpy as np
import traceback
from typing import Optional, List
from sklearn.cluster import KMeans

app = FastAPI()


# --- ìƒìˆ˜ ì •ì˜ ---
VEHICLE_CAPACITY_KG = 1200
DEFAULT_MAX_CAPA = 25
MIN_CALLS_SOFT = 10
DEFAULT_WEIGHT_KG = 15


# --- ë°ì´í„° ëª¨ë¸ ---
class Location(BaseModel):
    id: str
    lat: float
    lon: float
    weight: int = DEFAULT_WEIGHT_KG


class Driver(BaseModel):
    id: str
    name: str
    max_capa: Optional[int] = DEFAULT_MAX_CAPA
    base_lat: Optional[float] = None
    base_lng: Optional[float] = None
    vehicle_capacity_kg: Optional[int] = VEHICLE_CAPACITY_KG


class RequestBody(BaseModel):
    locations: List[Location]
    drivers: Optional[List[Driver]] = None
    num_vehicles: int = 4
    vehicle_capacity: int = VEHICLE_CAPACITY_KG


# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def haversine(lat1, lon1, lat2, lon2):
    """ë‘ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (km)"""
    R = 6371
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    dphi = np.radians(lat2 - lat1)
    dlambda = np.radians(lon2 - lon1)
    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2) * np.sin(dlambda/2)**2
    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))


def create_distance_matrix(df):
    """ê±°ë¦¬ í–‰ë ¬ ìƒì„± (ë¯¸í„° ë‹¨ìœ„ ì •ìˆ˜)"""
    n = len(df)
    coords = df[['lat', 'lon']].values
    dist_matrix = np.zeros((n, n), dtype=int)
    
    for i in range(n):
        for j in range(n):
            if i != j:
                dist_km = haversine(coords[i][0], coords[i][1],
                                   coords[j][0], coords[j][1])
                dist_matrix[i][j] = int(dist_km * 1000)
    
    return dist_matrix


def calculate_cluster_centroid(df, assignments, vehicle_id):
    """íŠ¹ì • í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ì  ê³„ì‚°"""
    assigned_indices = [idx for idx, vid in assignments.items() if vid == vehicle_id]
    if not assigned_indices:
        return None
    
    lats = [float(df.iloc[idx]['lat']) for idx in assigned_indices]
    lons = [float(df.iloc[idx]['lon']) for idx in assigned_indices]
    
    return (np.mean(lats), np.mean(lons))


def get_cluster_stats(df, assignments, vehicle_id):
    """íŠ¹ì • í´ëŸ¬ìŠ¤í„° í†µê³„"""
    assigned_indices = [idx for idx, vid in assignments.items() if vid == vehicle_id]
    
    total_weight = sum(int(df.iloc[idx]['weight']) for idx in assigned_indices)
    call_count = len(assigned_indices)
    
    return {
        'indices': list(assigned_indices),
        'call_count': call_count,
        'total_weight': total_weight
    }


def create_geographic_clusters(df, num_clusters, depot_idx=0):
    """
    â˜… V10.3 ì‹ ê·œ: K-Means ê¸°ë°˜ ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ â˜…
    
    ëª©ì : 
    - ì§€ë¦¬ì ìœ¼ë¡œ ê°€ê¹Œìš´ ë…¸ë“œë“¤ì„ ê°™ì€ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ìŒ
    - OR-Toolsì— "ê°™ì€ ê·¸ë£¹ì€ ê°™ì€ ì°¨ëŸ‰" íŒíŠ¸ ì œê³µ
    
    Returns:
        node_to_cluster: {node_idx: cluster_id} ë§¤í•‘
    """
    # depot ì œì™¸í•œ ì¢Œí‘œ ì¶”ì¶œ
    coords = []
    node_indices = []
    
    for i in range(len(df)):
        if i == depot_idx:
            continue
        coords.append([df.iloc[i]['lat'], df.iloc[i]['lon']])
        node_indices.append(i)
    
    if len(coords) < num_clusters:
        # ë…¸ë“œê°€ í´ëŸ¬ìŠ¤í„° ìˆ˜ë³´ë‹¤ ì ìœ¼ë©´ ê°ì ë³„ë„ í´ëŸ¬ìŠ¤í„°
        return {idx: i % num_clusters for i, idx in enumerate(node_indices)}
    
    # K-Means í´ëŸ¬ìŠ¤í„°ë§
    coords_array = np.array(coords)
    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(coords_array)
    
    # ê²°ê³¼ ë§¤í•‘
    node_to_cluster = {}
    for i, node_idx in enumerate(node_indices):
        node_to_cluster[node_idx] = int(labels[i])
    
    return node_to_cluster, kmeans.cluster_centers_


def match_clusters_to_drivers_v2(df, assignments, drivers, num_clusters, depot_idx=0):
    """
    V10.2: max_capaë¥¼ ê³ ë ¤í•œ í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­
    """
    
    # 1. í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„ ê³„ì‚°
    cluster_stats = {}
    for cluster_id in range(num_clusters):
        stats = get_cluster_stats(df, assignments, cluster_id)
        cluster_stats[cluster_id] = stats
        print(f"  í´ëŸ¬ìŠ¤í„° {cluster_id}: {stats['call_count']}ê±´, {stats['total_weight']}kg")
    
    # 2. í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê³„ì‚°
    cluster_centroids = {}
    for cluster_id in range(num_clusters):
        centroid = calculate_cluster_centroid(df, assignments, cluster_id)
        if centroid:
            cluster_centroids[cluster_id] = centroid
    
    # 3. ê¸°ì‚¬ ì •ë³´ ì •ë¦¬
    driver_info = {}
    for i, driver in enumerate(drivers):
        max_capa = driver.max_capa if driver.max_capa else DEFAULT_MAX_CAPA
        if driver.base_lat is not None and driver.base_lng is not None:
            base = (driver.base_lat, driver.base_lng)
        else:
            base = (float(df.iloc[depot_idx]['lat']), float(df.iloc[depot_idx]['lon']))
        
        driver_info[i] = {
            'name': driver.name,
            'max_capa': max_capa,
            'base': base
        }
        print(f"  ê¸°ì‚¬ {i} ({driver.name}): max_capa={max_capa}")
    
    # 4. max_capaë¥¼ ê³ ë ¤í•œ ë§¤ì¹­
    cluster_to_driver = {}
    used_drivers = set()
    
    # í´ëŸ¬ìŠ¤í„°ë¥¼ ì½œ ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_clusters = sorted(
        cluster_stats.keys(),
        key=lambda c: cluster_stats[c]['call_count'],
        reverse=True
    )
    
    print(f"\n  í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬ ìˆœì„œ (ì½œ ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ): {sorted_clusters}")
    
    for cluster_id in sorted_clusters:
        if cluster_id not in cluster_centroids:
            continue
        
        cluster_calls = cluster_stats[cluster_id]['call_count']
        centroid = cluster_centroids[cluster_id]
        
        # ì í•©í•œ ê¸°ì‚¬ í›„ë³´ ì°¾ê¸°
        candidates = []
        for driver_id, info in driver_info.items():
            if driver_id in used_drivers:
                continue
            
            if info['max_capa'] >= cluster_calls:
                dist = haversine(centroid[0], centroid[1], info['base'][0], info['base'][1])
                candidates.append((driver_id, dist, info['max_capa']))
        
        if candidates:
            candidates.sort(key=lambda x: x[1])
            best_driver = candidates[0][0]
            print(f"  í´ëŸ¬ìŠ¤í„° {cluster_id} ({cluster_calls}ê±´) â†’ ê¸°ì‚¬ {best_driver} ({driver_info[best_driver]['name']}, max_capa={driver_info[best_driver]['max_capa']}) [ê±°ë¦¬ ê¸°ë°˜]")
        else:
            remaining = [(d, info['max_capa']) for d, info in driver_info.items() if d not in used_drivers]
            if remaining:
                remaining.sort(key=lambda x: -x[1])
                best_driver = remaining[0][0]
                print(f"  í´ëŸ¬ìŠ¤í„° {cluster_id} ({cluster_calls}ê±´) â†’ ê¸°ì‚¬ {best_driver} ({driver_info[best_driver]['name']}, max_capa={driver_info[best_driver]['max_capa']}) [ì°¨ì„ ì±…]")
            else:
                best_driver = 0
                print(f"  í´ëŸ¬ìŠ¤í„° {cluster_id} ({cluster_calls}ê±´) â†’ ê¸°ì‚¬ 0 [fallback]")
        
        cluster_to_driver[cluster_id] = best_driver
        used_drivers.add(best_driver)
    
    # ë§¤ì¹­ ì•ˆ ëœ í´ëŸ¬ìŠ¤í„° ì²˜ë¦¬
    remaining_drivers = set(range(len(drivers))) - used_drivers
    for cluster_id in range(num_clusters):
        if cluster_id not in cluster_to_driver:
            if remaining_drivers:
                cluster_to_driver[cluster_id] = remaining_drivers.pop()
            else:
                cluster_to_driver[cluster_id] = 0
    
    print(f"\n  ìµœì¢… ë§¤ì¹­ ê²°ê³¼: {cluster_to_driver}")
    return cluster_to_driver


def optimize_visit_order(df, assignments, cluster_id, depot_idx=0):
    """í´ëŸ¬ìŠ¤í„°ë³„ ë°©ë¬¸ ìˆœì„œ ìµœì í™” (Nearest Neighbor)"""
    assigned_indices = [idx for idx, vid in assignments.items() if vid == cluster_id and idx != depot_idx]
    
    if len(assigned_indices) <= 1:
        return assigned_indices
    
    try:
        depot_lat = float(df.iloc[depot_idx]['lat'])
        depot_lon = float(df.iloc[depot_idx]['lon'])
        
        visited = []
        remaining = set(assigned_indices)
        current_lat, current_lon = depot_lat, depot_lon
        
        while remaining:
            nearest = min(remaining, key=lambda idx: haversine(
                current_lat, current_lon,
                float(df.iloc[idx]['lat']), float(df.iloc[idx]['lon'])
            ))
            visited.append(nearest)
            remaining.remove(nearest)
            current_lat = float(df.iloc[nearest]['lat'])
            current_lon = float(df.iloc[nearest]['lon'])
        
        return visited
        
    except Exception as e:
        print(f"optimize_visit_order error: {e}")
        return assigned_indices


@app.get("/")
def read_root():
    return {
        "status": "active",
        "message": "VRP Engine V10.3 (Geographic Clustering + Same Route Constraint)",
        "features": [
            "drivers í•„ë“œ ì„ íƒì ",
            "ê¸°ì‚¬ë³„ max_capa í•˜ë“œìº¡",
            "â˜… V10.3: K-Means ì‚¬ì „ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì§€ë¦¬ì  ê·¸ë£¹í™”",
            "â˜… V10.3: Same Vehicle Constraintë¡œ í´ëŸ¬ìŠ¤í„° êµì°¨ ë°©ì§€",
            "max_capa ê³ ë ¤í•œ í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­"
        ],
        "changelog": {
            "v10.3": "K-Means + Same Vehicle Constraintë¡œ í´ëŸ¬ìŠ¤í„° ê°„ êµì°¨ ì œê±°",
            "v10.2": "í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­ ì‹œ max_capa ì œì•½ ì¶”ê°€"
        }
    }


@app.post("/optimize")
def optimize_routes(body: RequestBody):
    """
    OR-Tools CVRP + â˜… V10.3: ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ + Same Vehicle Constraint â˜…
    
    í•µì‹¬ ë³€ê²½:
    1. K-Meansë¡œ ë¨¼ì € ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„° ìƒì„±
    2. ê°™ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ë…¸ë“œë“¤ì— Same Vehicle Constraint ì ìš©
    3. OR-Toolsê°€ í´ëŸ¬ìŠ¤í„° ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ë©´ì„œ ìµœì í™”
    """
    
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        data = [loc.dict() for loc in body.locations]
        df = pd.DataFrame(data)
        df = df.reset_index(drop=True)
        
        num_locations = len(df)
        depot_idx = 0
        
        if num_locations < 2:
            return {"status": "error", "message": "Not enough locations"}
        
        # drivers ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
        if body.drivers and len(body.drivers) > 0:
            drivers = body.drivers
            num_vehicles = len(drivers)
            use_driver_features = True
        else:
            num_vehicles = body.num_vehicles
            drivers = [
                Driver(
                    id=f"driver_{i+1}",
                    name=f"ê¸°ì‚¬ {i+1}",
                    max_capa=DEFAULT_MAX_CAPA,
                    base_lat=None,
                    base_lng=None,
                    vehicle_capacity_kg=body.vehicle_capacity
                )
                for i in range(num_vehicles)
            ]
            use_driver_features = False
        
        # ê¸°ì‚¬ë³„ ì„¤ì • ì¶”ì¶œ
        driver_max_capas = []
        driver_kg_capas = []
        for driver in drivers:
            max_capa = driver.max_capa if driver.max_capa else DEFAULT_MAX_CAPA
            kg_capa = driver.vehicle_capacity_kg if driver.vehicle_capacity_kg else body.vehicle_capacity
            driver_max_capas.append(max_capa)
            driver_kg_capas.append(kg_capa)
        
        print(f"\n=== VRP V10.3 ìµœì í™” ì‹œì‘ ===")
        print(f"ì´ ìœ„ì¹˜: {num_locations}ê°œ, ê¸°ì‚¬: {num_vehicles}ëª…")
        print(f"ê¸°ì‚¬ë³„ max_capa: {driver_max_capas}")
        print(f"ì´ ìˆ˜ìš© ê°€ëŠ¥: {sum(driver_max_capas)}ê±´")
        
        # weight ì²˜ë¦¬
        if 'weight' not in df.columns:
            df['weight'] = DEFAULT_WEIGHT_KG
        df['weight'] = pd.to_numeric(df['weight'], errors='coerce').fillna(DEFAULT_WEIGHT_KG).astype(int)
        df.loc[depot_idx, 'weight'] = 0
        
        # 2. â˜… V10.3: K-Means ì‚¬ì „ í´ëŸ¬ìŠ¤í„°ë§ â˜…
        print(f"\n=== ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ (K-Means) ===")
        node_to_cluster, cluster_centers = create_geographic_clusters(df, num_vehicles, depot_idx)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë…¸ë“œ ìˆ˜ ì¶œë ¥
        cluster_node_counts = {}
        for node_idx, cluster_id in node_to_cluster.items():
            cluster_node_counts[cluster_id] = cluster_node_counts.get(cluster_id, 0) + 1
        print(f"K-Means í´ëŸ¬ìŠ¤í„°ë³„ ë…¸ë“œ ìˆ˜: {cluster_node_counts}")
        
        # 3. ê±°ë¦¬ í–‰ë ¬
        dist_matrix = create_distance_matrix(df)
        
        # 4. OR-Tools CVRP ì„¤ì •
        manager = pywrapcp.RoutingIndexManager(num_locations, num_vehicles, depot_idx)
        routing = pywrapcp.RoutingModel(manager)
        
        def distance_callback(from_index, to_index):
            from_node = manager.IndexToNode(from_index)
            to_node = manager.IndexToNode(to_index)
            return dist_matrix[from_node][to_node]
        
        transit_callback_index = routing.RegisterTransitCallback(distance_callback)
        routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)
        
        # ë¯¸ë°°ì • í˜ë„í‹° (ìš©ëŸ‰ ì´ˆê³¼ ì‹œ ì¼ë¶€ ë¯¸ë°°ì • í—ˆìš©)
        UNASSIGNED_PENALTY = 10000000000
        for node_idx in range(1, num_locations):
            index = manager.NodeToIndex(node_idx)
            routing.AddDisjunction([index], UNASSIGNED_PENALTY)
        
        # ì ì¬ëŸ‰ ì œí•œ (ê¸°ì‚¬ë³„)
        def demand_callback(from_index):
            from_node = manager.IndexToNode(from_index)
            return int(df.iloc[from_node]['weight'])
        
        demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)
        routing.AddDimensionWithVehicleCapacity(
            demand_callback_index, 0,
            driver_kg_capas,
            True, 'Capacity'
        )
        
        # ì½œ ìˆ˜ ì œí•œ (ê¸°ì‚¬ë³„ max_capa í•˜ë“œìº¡)
        def count_callback(from_index):
            from_node = manager.IndexToNode(from_index)
            return 0 if from_node == depot_idx else 1
        
        count_callback_index = routing.RegisterUnaryTransitCallback(count_callback)
        routing.AddDimensionWithVehicleCapacity(
            count_callback_index, 0,
            driver_max_capas,
            True, 'CallCount'
        )
        
        # 5. â˜… V10.3 í•µì‹¬: Same Vehicle Constraint â˜…
        # ê°™ì€ K-Means í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ë…¸ë“œë“¤ì€ ê°™ì€ ì°¨ëŸ‰ì— ë°°ì •ë˜ë„ë¡ ê°•ì œ
        print(f"\n=== Same Vehicle Constraint ì ìš© ===")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë…¸ë“œ ì¸ë±ìŠ¤ ê·¸ë£¹í™”
        cluster_to_nodes = {}
        for node_idx, cluster_id in node_to_cluster.items():
            if cluster_id not in cluster_to_nodes:
                cluster_to_nodes[cluster_id] = []
            cluster_to_nodes[cluster_id].append(node_idx)
        
        # â˜… í•µì‹¬: ê° í´ëŸ¬ìŠ¤í„° ë‚´ ì¸ì ‘ ë…¸ë“œìŒì— Same Vehicle Constraint ì ìš© â˜…
        # ì „ì²´ ë…¸ë“œì— ì ìš©í•˜ë©´ ë„ˆë¬´ ë¹¡ë¹¡í•´ì„œ, í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ê¹Œìš´ "í•µì‹¬ ë…¸ë“œ"ë“¤ë§Œ ì—°ê²°
        constraints_added = 0
        
        for cluster_id, nodes in cluster_to_nodes.items():
            if len(nodes) < 2:
                continue
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì 
            center = cluster_centers[cluster_id]
            
            # ì¤‘ì‹¬ì— ê°€ê¹Œìš´ ìˆœìœ¼ë¡œ ì •ë ¬
            nodes_with_dist = []
            for node_idx in nodes:
                node_lat = float(df.iloc[node_idx]['lat'])
                node_lon = float(df.iloc[node_idx]['lon'])
                dist = haversine(node_lat, node_lon, center[0], center[1])
                nodes_with_dist.append((node_idx, dist))
            
            nodes_with_dist.sort(key=lambda x: x[1])
            
            # ìƒìœ„ 70% ë…¸ë“œë¥¼ "í•µì‹¬ ë…¸ë“œ"ë¡œ ì„ ì • (ë„ˆë¬´ ì™¸ê³½ì€ ìœ ì—°í•˜ê²Œ)
            core_count = max(2, int(len(nodes) * 0.7))
            core_nodes = [n[0] for n in nodes_with_dist[:core_count]]
            
            # í•µì‹¬ ë…¸ë“œë“¤ ê°„ì— Same Vehicle Constraint ì ìš©
            # ì²´ì¸ ë°©ì‹: A-B, B-C, C-D... (ëª¨ë“  ìŒ ì—°ê²°í•˜ë©´ ê³¼ë„í•œ ì œì•½)
            for i in range(len(core_nodes) - 1):
                node_a = core_nodes[i]
                node_b = core_nodes[i + 1]
                
                index_a = manager.NodeToIndex(node_a)
                index_b = manager.NodeToIndex(node_b)
                
                # Same Vehicle Constraint: ë‘ ë…¸ë“œê°€ ê°™ì€ ì°¨ëŸ‰ì— ë°°ì •ë˜ì–´ì•¼ í•¨
                routing.AddPickupAndDelivery(index_a, index_b)
                routing.solver().Add(
                    routing.VehicleVar(index_a) == routing.VehicleVar(index_b)
                )
                constraints_added += 1
        
        print(f"Same Vehicle Constraints ì¶”ê°€: {constraints_added}ê°œ")
        
        # ì½œ ìˆ˜ í•˜í•œ (ì†Œí”„íŠ¸)
        count_dimension = routing.GetDimensionOrDie('CallCount')
        CALL_PENALTY = 50000
        
        for vehicle_id in range(num_vehicles):
            end_index = routing.End(vehicle_id)
            count_dimension.SetCumulVarSoftLowerBound(end_index, MIN_CALLS_SOFT, CALL_PENALTY)
        
        # ê±°ë¦¬ ê· ë“±í™”
        routing.AddDimension(transit_callback_index, 0, 10000000, True, 'Distance')
        distance_dimension = routing.GetDimensionOrDie('Distance')
        distance_dimension.SetGlobalSpanCostCoefficient(200)
        
        # 6. ê²€ìƒ‰ ì „ëµ
        search_parameters = pywrapcp.DefaultRoutingSearchParameters()
        search_parameters.first_solution_strategy = (
            routing_enums_pb2.FirstSolutionStrategy.PARALLEL_CHEAPEST_INSERTION)
        search_parameters.local_search_metaheuristic = (
            routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH)
        search_parameters.time_limit.seconds = 30
        
        solution = routing.SolveWithParameters(search_parameters)
        
        if not solution:
            # Same Vehicle Constraintê°€ ë„ˆë¬´ ë¹¡ë¹¡í•˜ë©´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
            # Fallback: ì œì•½ ì—†ì´ ë‹¤ì‹œ ì‹œë„
            print("âš ï¸ Same Vehicle Constraintë¡œ í•´ ì°¾ê¸° ì‹¤íŒ¨, Fallback ì‹œë„...")
            
            # ìƒˆë¡œìš´ ëª¨ë¸ ìƒì„± (ì œì•½ ì—†ì´)
            manager2 = pywrapcp.RoutingIndexManager(num_locations, num_vehicles, depot_idx)
            routing2 = pywrapcp.RoutingModel(manager2)
            
            transit_callback_index2 = routing2.RegisterTransitCallback(distance_callback)
            routing2.SetArcCostEvaluatorOfAllVehicles(transit_callback_index2)
            
            for node_idx in range(1, num_locations):
                index = manager2.NodeToIndex(node_idx)
                routing2.AddDisjunction([index], UNASSIGNED_PENALTY)
            
            demand_callback_index2 = routing2.RegisterUnaryTransitCallback(demand_callback)
            routing2.AddDimensionWithVehicleCapacity(
                demand_callback_index2, 0, driver_kg_capas, True, 'Capacity')
            
            count_callback_index2 = routing2.RegisterUnaryTransitCallback(count_callback)
            routing2.AddDimensionWithVehicleCapacity(
                count_callback_index2, 0, driver_max_capas, True, 'CallCount')
            
            solution = routing2.SolveWithParameters(search_parameters)
            manager = manager2
            routing = routing2
            
            if not solution:
                return {
                    "status": "fail",
                    "message": "Solution not found. ì ì¬ëŸ‰/ì½œìˆ˜ ì œì•½ í™•ì¸ í•„ìš”.",
                    "debug": {
                        "total_calls": num_locations - 1,
                        "total_max_capa": sum(driver_max_capas),
                        "driver_max_capas": driver_max_capas
                    }
                }
        
        # 7. í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ
        cluster_assignments = {}
        
        for cluster_id in range(num_vehicles):
            index = routing.Start(cluster_id)
            while not routing.IsEnd(index):
                node_idx = manager.IndexToNode(index)
                if node_idx != depot_idx:
                    cluster_assignments[node_idx] = cluster_id
                index = solution.Value(routing.NextVar(index))
        
        # 8. í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­ (V10.2 ë¡œì§)
        print("\n=== í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­ (V10.2) ===")
        if use_driver_features:
            cluster_to_driver = match_clusters_to_drivers_v2(
                df, cluster_assignments, drivers, num_vehicles, depot_idx)
        else:
            cluster_to_driver = {i: i for i in range(num_vehicles)}
        
        # 9. ê²°ê³¼ ìƒì„±
        results = []
        stats = []
        total_distance = 0
        
        for cluster_id in range(num_vehicles):
            driver_id = cluster_to_driver.get(cluster_id, cluster_id)
            driver = drivers[driver_id] if driver_id < len(drivers) else drivers[0]
            
            visit_order = optimize_visit_order(df, cluster_assignments, cluster_id, depot_idx)
            
            route_distance = 0
            route_weight = 0
            prev_lat = float(df.iloc[depot_idx]['lat'])
            prev_lon = float(df.iloc[depot_idx]['lon'])
            
            for order, node_idx in enumerate(visit_order, 1):
                try:
                    node = df.iloc[node_idx]
                    node_weight = int(node['weight'])
                    route_weight += node_weight
                    route_distance += haversine(prev_lat, prev_lon, 
                                               float(node['lat']), float(node['lon']))
                    prev_lat, prev_lon = float(node['lat']), float(node['lon'])
                    
                    results.append({
                        "id": str(node['id']),
                        "driver_id": driver.id,
                        "driver_name": driver.name,
                        "visit_order": order,
                        "weight_kg": node_weight,
                        "cumulative_weight_kg": route_weight
                    })
                except Exception as e:
                    print(f"Error processing node {node_idx}: {e}")
                    continue
            
            if visit_order:
                route_distance += haversine(prev_lat, prev_lon,
                                           float(df.iloc[depot_idx]['lat']),
                                           float(df.iloc[depot_idx]['lon']))
            
            total_distance += route_distance
            
            call_count = len(visit_order)
            max_capa = driver.max_capa if driver.max_capa else DEFAULT_MAX_CAPA
            
            status = "ì •ìƒ"
            if call_count < MIN_CALLS_SOFT:
                status = f"âš ï¸ í•˜í•œ ë¯¸ë‹¬ ({call_count} < {MIN_CALLS_SOFT})"
            elif call_count > max_capa:
                status = f"ğŸš¨ ìƒí•œ ì´ˆê³¼ ({call_count} > {max_capa})"
            
            cluster_centroid = calculate_cluster_centroid(df, cluster_assignments, cluster_id)
            base_distance = 0
            if cluster_centroid and driver.base_lat and driver.base_lng:
                base_distance = haversine(cluster_centroid[0], cluster_centroid[1],
                                         driver.base_lat, driver.base_lng)
            
            stats.append({
                "driver_id": driver.id,
                "driver_name": driver.name,
                "call_count": call_count,
                "max_capa": max_capa,
                "total_weight_kg": route_weight,
                "vehicle_capacity_kg": driver.vehicle_capacity_kg or body.vehicle_capacity,
                "distance_km": round(route_distance, 2),
                "base_to_cluster_km": round(base_distance, 2),
                "status": status
            })
        
        # ë¯¸ë°°ì • ì²´í¬
        assigned_ids = set([r['id'] for r in results])
        all_ids = set(str(df.iloc[i]['id']) for i in range(1, len(df)))
        unassigned_ids = all_ids - assigned_ids
        
        # í´ëŸ¬ìŠ¤í„° êµì°¨ ê²€ì¦
        cluster_overlap = check_cluster_overlap(df, cluster_assignments, num_vehicles)
        
        print(f"\n=== ìµœì í™” ì™„ë£Œ ===")
        print(f"ë°°ì •: {len(results)}ê±´, ë¯¸ë°°ì •: {len(unassigned_ids)}ê±´")
        print(f"í´ëŸ¬ìŠ¤í„° êµì°¨: {cluster_overlap}")
        
        return {
            "status": "success",
            "updates": results,
            "statistics": stats,
            "summary": {
                "total_locations": num_locations - 1,
                "total_assigned": len(results),
                "unassigned": len(unassigned_ids),
                "unassigned_ids": list(unassigned_ids) if unassigned_ids else [],
                "total_distance_km": round(total_distance, 2),
                "avg_distance_km": round(total_distance / num_vehicles, 2) if num_vehicles > 0 else 0
            },
            "optimization_info": {
                "algorithm": "V10.3: K-Means + Same Vehicle Constraint",
                "same_vehicle_constraints": constraints_added,
                "cluster_overlap_score": cluster_overlap,
                "use_driver_features": use_driver_features
            },
            "matching_info": {
                "cluster_to_driver": {f"í´ëŸ¬ìŠ¤í„°{k}": drivers[v].name for k, v in cluster_to_driver.items()},
                "algorithm": "V10.2: max_capa ê³ ë ¤ ë§¤ì¹­"
            } if use_driver_features else None
        }
        
    except Exception as e:
        traceback.print_exc()
        return {
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }


def check_cluster_overlap(df, assignments, num_clusters):
    """
    í´ëŸ¬ìŠ¤í„° ê°„ ì§€ë¦¬ì  êµì°¨ ì •ë„ ì¸¡ì •
    
    ë°©ë²•: ê° ë…¸ë“œê°€ ìì‹ ì˜ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ë³´ë‹¤ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ë” ê°€ê¹Œìš´ ë¹„ìœ¨
    ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (0% = ì™„ë²½í•œ ë¶„ë¦¬)
    """
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê³„ì‚°
    centroids = {}
    for cluster_id in range(num_clusters):
        centroid = calculate_cluster_centroid(df, assignments, cluster_id)
        if centroid:
            centroids[cluster_id] = centroid
    
    if len(centroids) < 2:
        return 0.0
    
    overlap_count = 0
    total_count = 0
    
    for node_idx, assigned_cluster in assignments.items():
        if assigned_cluster not in centroids:
            continue
        
        node_lat = float(df.iloc[node_idx]['lat'])
        node_lon = float(df.iloc[node_idx]['lon'])
        
        own_centroid = centroids[assigned_cluster]
        own_dist = haversine(node_lat, node_lon, own_centroid[0], own_centroid[1])
        
        # ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ ì¤‘ ìµœì†Œê°’
        min_other_dist = float('inf')
        for other_cluster, other_centroid in centroids.items():
            if other_cluster == assigned_cluster:
                continue
            other_dist = haversine(node_lat, node_lon, other_centroid[0], other_centroid[1])
            min_other_dist = min(min_other_dist, other_dist)
        
        # ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì´ ë” ê°€ê¹Œìš°ë©´ "êµì°¨"ë¡œ íŒì •
        if min_other_dist < own_dist:
            overlap_count += 1
        
        total_count += 1
    
    if total_count == 0:
        return 0.0
    
    return round(overlap_count / total_count * 100, 1)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
