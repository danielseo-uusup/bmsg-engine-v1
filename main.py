from fastapi import FastAPI
from pydantic import BaseModel
import pandas as pd
import numpy as np
import traceback
from typing import Optional, List

app = FastAPI()


# --- ìƒìˆ˜ ì •ì˜ ---
VEHICLE_CAPACITY_KG = 1200
DEFAULT_MAX_CAPA = 25
DEFAULT_WEIGHT_KG = 15


# --- ë°ì´í„° ëª¨ë¸ ---
class Location(BaseModel):
    id: str
    lat: float
    lon: float
    weight: int = DEFAULT_WEIGHT_KG


class Driver(BaseModel):
    id: str
    name: str
    max_capa: Optional[int] = DEFAULT_MAX_CAPA
    base_lat: Optional[float] = None
    base_lng: Optional[float] = None
    vehicle_capacity_kg: Optional[int] = VEHICLE_CAPACITY_KG


class RequestBody(BaseModel):
    locations: List[Location]
    drivers: Optional[List[Driver]] = None
    num_vehicles: int = 4
    vehicle_capacity: int = VEHICLE_CAPACITY_KG


# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def haversine(lat1, lon1, lat2, lon2):
    """ë‘ ì¢Œí‘œ ê°„ ê±°ë¦¬ ê³„ì‚° (km)"""
    R = 6371
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    dphi = np.radians(lat2 - lat1)
    dlambda = np.radians(lon2 - lon1)
    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2) * np.sin(dlambda/2)**2
    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))


def kmeans_clustering(coords, n_clusters, max_iter=100):
    """
    ìˆœìˆ˜ numpyë¡œ êµ¬í˜„í•œ K-Means í´ëŸ¬ìŠ¤í„°ë§
    sklearn ì—†ì´ ë™ì‘
    """
    n_samples = len(coords)
    
    if n_samples <= n_clusters:
        return list(range(n_samples))
    
    # ì´ˆê¸° ì¤‘ì‹¬ì : ëœë¤ ì„ íƒ
    np.random.seed(42)
    indices = np.random.choice(n_samples, n_clusters, replace=False)
    centroids = coords[indices].copy()
    
    labels = np.zeros(n_samples, dtype=int)
    
    for _ in range(max_iter):
        # ê° ì ì„ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì— í• ë‹¹
        new_labels = np.zeros(n_samples, dtype=int)
        for i in range(n_samples):
            distances = [haversine(coords[i][0], coords[i][1], c[0], c[1]) for c in centroids]
            new_labels[i] = np.argmin(distances)
        
        # ìˆ˜ë ´ ì²´í¬
        if np.array_equal(labels, new_labels):
            break
        labels = new_labels
        
        # ì¤‘ì‹¬ì  ì—…ë°ì´íŠ¸
        for k in range(n_clusters):
            cluster_points = coords[labels == k]
            if len(cluster_points) > 0:
                centroids[k] = cluster_points.mean(axis=0)
    
    return labels.tolist(), centroids


def geographic_clustering(df, n_clusters, depot_idx=0):
    """
    â˜… V11.3 í•µì‹¬: ë…¸ë“œ ë¶„í¬ ê¸°ë°˜ ì§€ë¦¬ì  í´ëŸ¬ìŠ¤í„°ë§ â˜…
    
    ì›ì¹™:
    1. ê¸°ì‚¬ ê±°ì  ë¬´ì‹œ, ì˜¤ì§ ë…¸ë“œ ìœ„ì¹˜ë§Œ ê³ ë ¤
    2. K-Meansë¡œ ì§€ë¦¬ì ìœ¼ë¡œ ê°€ê¹Œìš´ ë…¸ë“œë“¤ì„ ë¬¶ìŒ
    3. ê²°ê³¼: ì„œë¡œ ê²¹ì¹˜ì§€ ì•ŠëŠ” ëª…í™•í•œ í´ëŸ¬ìŠ¤í„°
    """
    
    print("\n=== 1ë‹¨ê³„: ë…¸ë“œ ë¶„í¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ ===")
    
    # ë…¸ë“œ ì¢Œí‘œ ì¶”ì¶œ (depot ì œì™¸)
    coords = []
    node_indices = []
    
    for i in range(len(df)):
        if i == depot_idx:
            continue
        coords.append([float(df.iloc[i]['lat']), float(df.iloc[i]['lon'])])
        node_indices.append(i)
    
    coords = np.array(coords)
    
    # K-Means í´ëŸ¬ìŠ¤í„°ë§
    labels, centroids = kmeans_clustering(coords, n_clusters)
    
    # ê²°ê³¼ ì •ë¦¬
    cluster_nodes = {i: [] for i in range(n_clusters)}
    for i, node_idx in enumerate(node_indices):
        cluster_id = labels[i]
        cluster_nodes[cluster_id].append(node_idx)
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ì •ë³´
    cluster_info = []
    for cluster_id in range(n_clusters):
        nodes = cluster_nodes[cluster_id]
        if nodes:
            center_lat = centroids[cluster_id][0]
            center_lon = centroids[cluster_id][1]
        else:
            center_lat, center_lon = 0, 0
        
        cluster_info.append({
            'cluster_id': cluster_id,
            'node_count': len(nodes),
            'center_lat': center_lat,
            'center_lon': center_lon
        })
        print(f"  í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(nodes)}ê°œ ë…¸ë“œ, ì¤‘ì‹¬=({center_lat:.4f}, {center_lon:.4f})")
    
    return cluster_nodes, cluster_info, centroids


def match_clusters_to_drivers(cluster_info, drivers, df, depot_idx=0):
    """
    â˜… V11.3: í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ìµœì  ë§¤ì¹­ â˜…
    
    ì›ì¹™:
    1. í° í´ëŸ¬ìŠ¤í„° â†’ max_capa í° ê¸°ì‚¬
    2. ê¸°ì‚¬ ê±°ì ê³¼ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê±°ë¦¬ëŠ” ë³´ì¡° ê¸°ì¤€
    
    ì•Œê³ ë¦¬ì¦˜: í¬ê¸° ìš°ì„  Greedy ë§¤ì¹­
    """
    
    print("\n=== 2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­ ===")
    
    # ê¸°ì‚¬ ì •ë³´ ì •ë¦¬
    driver_info = []
    for i, driver in enumerate(drivers):
        max_capa = driver.max_capa if driver.max_capa else DEFAULT_MAX_CAPA
        base_lat = driver.base_lat if driver.base_lat else float(df.iloc[depot_idx]['lat'])
        base_lng = driver.base_lng if driver.base_lng else float(df.iloc[depot_idx]['lon'])
        
        driver_info.append({
            'driver_idx': i,
            'driver': driver,
            'max_capa': max_capa,
            'base_lat': base_lat,
            'base_lng': base_lng
        })
    
    # í´ëŸ¬ìŠ¤í„°ë¥¼ ë…¸ë“œ ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_clusters = sorted(cluster_info, key=lambda x: -x['node_count'])
    
    # ê¸°ì‚¬ë¥¼ max_capa ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_drivers = sorted(driver_info, key=lambda x: -x['max_capa'])
    
    print(f"  í´ëŸ¬ìŠ¤í„° í¬ê¸°: {[c['node_count'] for c in sorted_clusters]}")
    print(f"  ê¸°ì‚¬ max_capa: {[d['max_capa'] for d in sorted_drivers]}")
    
    # 1:1 ë§¤ì¹­ (í¬ê¸°ìˆœ)
    cluster_to_driver = {}
    
    for i, c_info in enumerate(sorted_clusters):
        if i < len(sorted_drivers):
            d_info = sorted_drivers[i]
            cluster_to_driver[c_info['cluster_id']] = d_info['driver_idx']
            print(f"  í´ëŸ¬ìŠ¤í„° {c_info['cluster_id']} ({c_info['node_count']}ê±´) â†’ {d_info['driver'].name} (max_capa={d_info['max_capa']})")
    
    return cluster_to_driver, driver_info


def balance_clusters_by_capacity(df, cluster_nodes, cluster_info, cluster_to_driver, driver_info, depot_idx=0):
    """
    â˜… V11.3: max_capa ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ê²½ê³„ ì¡°ì • â˜…
    
    í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ìˆ˜ > ê¸°ì‚¬ max_capaì¸ ê²½ìš°:
    1. í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì—ì„œ ê°€ì¥ ë¨¼ ë…¸ë“œ (ê²½ê³„ ë…¸ë“œ) ì°¾ê¸°
    2. ì¸ì ‘ í´ëŸ¬ìŠ¤í„° ì¤‘ ì—¬ìœ  ìˆëŠ” ê³³ìœ¼ë¡œ ì´ì „
    3. ì´ì „ ì‹œ "ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬"ìœ¼ë¡œ ì´ë™ (ì§€ë¦¬ì  ì—°ì†ì„± ìœ ì§€)
    """
    
    print("\n=== 3ë‹¨ê³„: max_capa ê¸°ë°˜ ê²½ê³„ ì¡°ì • ===")
    
    # í˜„ì¬ ìƒíƒœ ë³µì‚¬
    balanced = {k: list(v) for k, v in cluster_nodes.items()}
    
    # ê¸°ì‚¬ë³„ max_capa
    driver_max_capa = {d['driver_idx']: d['max_capa'] for d in driver_info}
    
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì 
    cluster_centers = {c['cluster_id']: (c['center_lat'], c['center_lon']) for c in cluster_info}
    
    max_iterations = 100
    total_moved = 0
    
    for iteration in range(max_iterations):
        moved = False
        
        for cluster_id, nodes in list(balanced.items()):
            if cluster_id not in cluster_to_driver:
                continue
            
            driver_idx = cluster_to_driver[cluster_id]
            max_capa = driver_max_capa[driver_idx]
            excess = len(nodes) - max_capa
            
            if excess <= 0:
                continue
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬
            center = cluster_centers[cluster_id]
            
            # ì¤‘ì‹¬ì—ì„œ ê°€ì¥ ë¨¼ ë…¸ë“œë“¤ (ê²½ê³„ ë…¸ë“œ)
            nodes_with_dist = []
            for node_idx in nodes:
                node_lat = float(df.iloc[node_idx]['lat'])
                node_lon = float(df.iloc[node_idx]['lon'])
                dist = haversine(center[0], center[1], node_lat, node_lon)
                nodes_with_dist.append((node_idx, dist, node_lat, node_lon))
            
            nodes_with_dist.sort(key=lambda x: -x[1])  # ê±°ë¦¬ ë‚´ë¦¼ì°¨ìˆœ
            
            # ì´ˆê³¼ë¶„ ì´ì „
            for node_idx, _, node_lat, node_lon in nodes_with_dist[:excess]:
                # ì—¬ìœ  ìˆëŠ” í´ëŸ¬ìŠ¤í„° ì¤‘ í•´ë‹¹ ë…¸ë“œì™€ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ ì°¾ê¸°
                best_target = None
                best_dist = float('inf')
                
                for other_cluster_id, other_nodes in balanced.items():
                    if other_cluster_id == cluster_id:
                        continue
                    if other_cluster_id not in cluster_to_driver:
                        continue
                    
                    other_driver_idx = cluster_to_driver[other_cluster_id]
                    other_max_capa = driver_max_capa[other_driver_idx]
                    
                    # ì—¬ìœ  í™•ì¸
                    if len(other_nodes) >= other_max_capa:
                        continue
                    
                    # í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬
                    other_center = cluster_centers[other_cluster_id]
                    dist = haversine(node_lat, node_lon, other_center[0], other_center[1])
                    
                    if dist < best_dist:
                        best_dist = dist
                        best_target = other_cluster_id
                
                if best_target is not None:
                    balanced[cluster_id].remove(node_idx)
                    balanced[best_target].append(node_idx)
                    moved = True
                    total_moved += 1
        
        if not moved:
            break
    
    print(f"  ì´ {total_moved}ê°œ ë…¸ë“œ ì´ì „")
    
    # ìµœì¢… ìƒíƒœ
    print(f"\n  ì¡°ì • í›„ í´ëŸ¬ìŠ¤í„°ë³„ ë…¸ë“œ ìˆ˜:")
    for cluster_id, nodes in balanced.items():
        if cluster_id in cluster_to_driver:
            driver_idx = cluster_to_driver[cluster_id]
            d_info = next(d for d in driver_info if d['driver_idx'] == driver_idx)
            max_capa = d_info['max_capa']
            status = "âœ…" if len(nodes) <= max_capa else "âŒ"
            print(f"    í´ëŸ¬ìŠ¤í„° {cluster_id} â†’ {d_info['driver'].name}: {len(nodes)}ê±´ (max={max_capa}) {status}")
    
    return balanced


def handle_overflow(df, balanced, cluster_to_driver, driver_info, cluster_info):
    """
    ì´ ë…¸ë“œ > ì´ max_capaì¸ ê²½ìš° ë¯¸ë°°ì • ì²˜ë¦¬
    ê¸°ì¤€: í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì—ì„œ ê°€ì¥ ë¨¼ ë…¸ë“œë“¤
    """
    
    print("\n=== 4ë‹¨ê³„: ì´ˆê³¼ë¶„ ì²˜ë¦¬ ===")
    
    total_max_capa = sum(d['max_capa'] for d in driver_info)
    total_nodes = sum(len(nodes) for nodes in balanced.values())
    overflow = total_nodes - total_max_capa
    
    if overflow <= 0:
        print(f"  ì´ˆê³¼ ì—†ìŒ")
        return balanced, []
    
    print(f"  {overflow}ê±´ ë¯¸ë°°ì • í•„ìš”")
    
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬
    cluster_centers = {c['cluster_id']: (c['center_lat'], c['center_lon']) for c in cluster_info}
    
    # ëª¨ë“  ë…¸ë“œì˜ "í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬" ê³„ì‚°
    all_nodes = []
    for cluster_id, nodes in balanced.items():
        if cluster_id not in cluster_centers:
            continue
        center = cluster_centers[cluster_id]
        
        for node_idx in nodes:
            node_lat = float(df.iloc[node_idx]['lat'])
            node_lon = float(df.iloc[node_idx]['lon'])
            dist = haversine(center[0], center[1], node_lat, node_lon)
            all_nodes.append({
                'node_idx': node_idx,
                'cluster_id': cluster_id,
                'dist': dist
            })
    
    # ê±°ë¦¬ ë‚´ë¦¼ì°¨ìˆœ (ë¨¼ ê²ƒë¶€í„° ì œê±°)
    all_nodes.sort(key=lambda x: -x['dist'])
    
    unassigned = []
    for node_info in all_nodes[:overflow]:
        node_idx = node_info['node_idx']
        cluster_id = node_info['cluster_id']
        
        if node_idx in balanced[cluster_id]:
            balanced[cluster_id].remove(node_idx)
            unassigned.append(node_idx)
    
    print(f"  {len(unassigned)}ê±´ ë¯¸ë°°ì •")
    
    return balanced, unassigned


def optimize_visit_order(df, nodes, start_lat, start_lon):
    """Nearest Neighbor TSP"""
    if not nodes:
        return []
    if len(nodes) == 1:
        return list(nodes)
    
    visited = []
    remaining = set(nodes)
    current_lat, current_lon = start_lat, start_lon
    
    while remaining:
        nearest = min(remaining, key=lambda idx: haversine(
            current_lat, current_lon,
            float(df.iloc[idx]['lat']), float(df.iloc[idx]['lon'])
        ))
        visited.append(nearest)
        remaining.remove(nearest)
        current_lat = float(df.iloc[nearest]['lat'])
        current_lon = float(df.iloc[nearest]['lon'])
    
    return visited


def calculate_route_distance(df, visit_order, start_lat, start_lon):
    """ê²½ë¡œ ê±°ë¦¬ ê³„ì‚°"""
    if not visit_order:
        return 0
    
    total = 0
    current_lat, current_lon = start_lat, start_lon
    
    for node_idx in visit_order:
        node_lat = float(df.iloc[node_idx]['lat'])
        node_lon = float(df.iloc[node_idx]['lon'])
        total += haversine(current_lat, current_lon, node_lat, node_lon)
        current_lat, current_lon = node_lat, node_lon
    
    return total


@app.get("/")
def read_root():
    return {
        "status": "active",
        "message": "VRP Engine V11.3 (Node-Distribution Clustering)",
        "features": [
            "â˜… ë…¸ë“œ ë¶„í¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ (ê±°ì  ë¬´ì‹œ)",
            "â˜… K-Meansë¡œ ì§€ë¦¬ì ìœ¼ë¡œ ê°€ê¹Œìš´ ë…¸ë“œ ë¬¶ìŒ",
            "â˜… í´ëŸ¬ìŠ¤í„° ê°„ ê²¹ì¹¨/ê°ì‹¸ê¸° ì—†ìŒ",
            "â˜… í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­: í¬ê¸° â†’ max_capa",
            "max_capa ê²½ê³„ ì¡°ì •",
            "Nearest Neighbor TSP"
        ],
        "algorithm": "K-Means Geographic Clustering â†’ Size-based Driver Matching â†’ Boundary Adjustment"
    }


@app.post("/optimize")
def optimize_routes(body: RequestBody):
    """
    â˜… V11.3: ë…¸ë“œ ë¶„í¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ â˜…
    
    í•µì‹¬:
    1. ê¸°ì‚¬ ê±°ì  ë¬´ì‹œ, ë…¸ë“œ ìœ„ì¹˜ë§Œìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
    2. K-Meansë¡œ ì§€ë¦¬ì ìœ¼ë¡œ ëª…í™•í•œ ê²½ê³„ ìƒì„±
    3. í´ëŸ¬ìŠ¤í„° í¬ê¸°ì™€ ê¸°ì‚¬ max_capa ë§¤ì¹­
    """
    
    try:
        # 1. ë°ì´í„° ì¤€ë¹„
        data = [loc.dict() for loc in body.locations]
        df = pd.DataFrame(data)
        df = df.reset_index(drop=True)
        
        num_locations = len(df)
        depot_idx = 0
        
        if num_locations < 2:
            return {"status": "error", "message": "Not enough locations"}
        
        # ê¸°ì‚¬ ì„¤ì •
        if body.drivers and len(body.drivers) > 0:
            drivers = body.drivers
        else:
            drivers = [
                Driver(id=f"driver_{i+1}", name=f"ê¸°ì‚¬ {i+1}", max_capa=DEFAULT_MAX_CAPA)
                for i in range(body.num_vehicles)
            ]
        
        num_drivers = len(drivers)
        total_max_capa = sum(d.max_capa or DEFAULT_MAX_CAPA for d in drivers)
        total_calls = num_locations - 1
        
        print(f"\n{'='*50}")
        print(f"VRP V11.3 - Node-Distribution Clustering")
        print(f"{'='*50}")
        print(f"ì´ ì½œ: {total_calls}ê±´, ìˆ˜ìš©ëŸ‰: {total_max_capa}ê±´")
        
        if 'weight' not in df.columns:
            df['weight'] = DEFAULT_WEIGHT_KG
        df['weight'] = pd.to_numeric(df['weight'], errors='coerce').fillna(DEFAULT_WEIGHT_KG).astype(int)
        
        # 2. ë…¸ë“œ ë¶„í¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§
        cluster_nodes, cluster_info, centroids = geographic_clustering(df, num_drivers, depot_idx)
        
        # 3. í´ëŸ¬ìŠ¤í„°-ê¸°ì‚¬ ë§¤ì¹­
        cluster_to_driver, driver_info = match_clusters_to_drivers(cluster_info, drivers, df, depot_idx)
        
        # 4. max_capa ê²½ê³„ ì¡°ì •
        balanced = balance_clusters_by_capacity(
            df, cluster_nodes, cluster_info, cluster_to_driver, driver_info, depot_idx
        )
        
        # 5. ì´ˆê³¼ë¶„ ì²˜ë¦¬
        balanced, unassigned = handle_overflow(df, balanced, cluster_to_driver, driver_info, cluster_info)
        
        # 6. ê²°ê³¼ ìƒì„±
        print("\n=== 5ë‹¨ê³„: ë°©ë¬¸ ìˆœì„œ ìµœì í™” ===")
        
        results = []
        stats = []
        total_distance = 0
        
        driver_info_map = {d['driver_idx']: d for d in driver_info}
        
        for cluster_id, nodes in balanced.items():
            if cluster_id not in cluster_to_driver:
                continue
            
            driver_idx = cluster_to_driver[cluster_id]
            d_info = driver_info_map[driver_idx]
            driver = d_info['driver']
            max_capa = d_info['max_capa']
            base_lat = d_info['base_lat']
            base_lng = d_info['base_lng']
            
            if not nodes:
                stats.append({
                    "driver_id": driver.id,
                    "driver_name": driver.name,
                    "call_count": 0,
                    "max_capa": max_capa,
                    "total_weight_kg": 0,
                    "distance_km": 0,
                    "status": "âš ï¸ ë°°ì • ì—†ìŒ"
                })
                continue
            
            visit_order = optimize_visit_order(df, nodes, base_lat, base_lng)
            route_distance = calculate_route_distance(df, visit_order, base_lat, base_lng)
            total_distance += route_distance
            
            route_weight = 0
            for order, node_idx in enumerate(visit_order, 1):
                node = df.iloc[node_idx]
                node_weight = int(node['weight'])
                route_weight += node_weight
                
                results.append({
                    "id": str(node['id']),
                    "driver_id": driver.id,
                    "driver_name": driver.name,
                    "visit_order": order,
                    "weight_kg": node_weight,
                    "cumulative_weight_kg": route_weight
                })
            
            call_count = len(visit_order)
            status = "ì •ìƒ" if call_count <= max_capa else f"ğŸš¨ ì´ˆê³¼"
            
            stats.append({
                "driver_id": driver.id,
                "driver_name": driver.name,
                "call_count": call_count,
                "max_capa": max_capa,
                "total_weight_kg": route_weight,
                "distance_km": round(route_distance, 2),
                "status": status
            })
            
            print(f"  {driver.name}: {call_count}ê±´, {route_distance:.1f}km")
        
        violations = [s for s in stats if s['call_count'] > s['max_capa']]
        
        print(f"\n{'='*50}")
        print(f"ì™„ë£Œ: ë°°ì • {len(results)}ê±´, ë¯¸ë°°ì • {len(unassigned)}ê±´")
        print(f"{'='*50}")
        
        return {
            "status": "success",
            "updates": results,
            "statistics": stats,
            "summary": {
                "total_locations": total_calls,
                "total_assigned": len(results),
                "unassigned": len(unassigned),
                "unassigned_ids": [str(df.iloc[idx]['id']) for idx in unassigned],
                "total_distance_km": round(total_distance, 2)
            },
            "optimization_info": {
                "algorithm": "V11.3: Node-Distribution K-Means + Size-based Matching",
                "max_capa_violations": len(violations),
                "cluster_overlap": 0
            }
        }
        
    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
